{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the path to the zip file\n",
    "zip_file_path = '/content/human_vital_signs_dataset_2024.csv.zip'  # Updated path\n",
    "\n",
    "# Check if the zip file exists\n",
    "if not os.path.exists(zip_file_path):\n",
    "    print(f\"Error: zip file not found at {zip_file_path}\")\n",
    "else:\n",
    "    # Create a directory to extract the contents\n",
    "    extraction_path = '/extracted_data'\n",
    "    os.makedirs(extraction_path, exist_ok=True)\n",
    "\n",
    "    # Extract the contents of the zip file\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extraction_path)\n",
    "            extracted_files = zip_ref.namelist()  # Get a list of extracted file names\n",
    "        print(f\"Extracted {len(extracted_files)} files to {extraction_path}\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"Error: {zip_file_path} is not a valid zip file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during extraction: {e}\")\n",
    "\n",
    "    # After extraction, list files in the directory to find CSV files\n",
    "    if os.path.exists(extraction_path):\n",
    "        all_files = os.listdir(extraction_path)\n",
    "        csv_files = [f for f in all_files if f.lower().endswith('.csv')]\n",
    "\n",
    "        if not csv_files:\n",
    "            print(f\"Error: No CSV files found in the extracted directory {extraction_path}\")\n",
    "        else:\n",
    "            # Display head of the vital signs dataset\n",
    "            csv_file_path = os.path.join(extraction_path, csv_files[0])\n",
    "            try:\n",
    "                vital_signs_df = pd.read_csv(csv_file_path)\n",
    "                print(f\"\\n--- Head of Human Vital Signs Dataset --- (Path: {csv_file_path})\")\n",
    "                print(vital_signs_df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "                \n",
    "                # Display basic info about the dataset\n",
    "                print(f\"\\n--- Dataset Info ---\")\n",
    "                print(f\"Number of rows: {vital_signs_df.shape[0]}\")\n",
    "                print(f\"Number of columns: {vital_signs_df.shape[1]}\")\n",
    "                print(f\"Columns: {', '.join(vital_signs_df.columns)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading CSV file ({csv_files[0]}): {e}\")\n",
    "    else:\n",
    "        print(\"Extraction directory not found. Please check the extraction process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Load the dataset\n",
    "input_path = \"/extracted_data/human_vital_signs_dataset_2024.csv\"\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Process patients\n",
    "transformed_df = pd.DataFrame(columns=df.columns)\n",
    "test_patients = 10\n",
    "\n",
    "for patient_idx in range(test_patients):\n",
    "    base_idx = patient_idx * 10\n",
    "    if base_idx >= len(df):\n",
    "        break\n",
    "\n",
    "    first_row = df.iloc[base_idx].copy()\n",
    "    gender = first_row['Gender']\n",
    "    base_timestamp = datetime.strptime(first_row['Timestamp'], '%Y-%m-%d %H:%M:%S.%f')\n",
    "    base_age = first_row['Age']\n",
    "\n",
    "    for year_offset in range(10):\n",
    "        idx = base_idx + year_offset\n",
    "        if idx < len(df):\n",
    "            row = df.iloc[idx].copy()\n",
    "\n",
    "            row['Patient ID'] = first_row['Patient ID']\n",
    "            row['Gender'] = gender\n",
    "\n",
    "            new_year = base_timestamp.year + year_offset\n",
    "            new_timestamp = base_timestamp.replace(year=new_year)\n",
    "            row['Timestamp'] = new_timestamp.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "            row['Age'] = base_age + year_offset\n",
    "\n",
    "            transformed_df = pd.concat([transformed_df, pd.DataFrame(row).T], ignore_index=True)\n",
    "\n",
    "# Remove the Height (m) column\n",
    "transformed_df = transformed_df.drop(columns=['Height (m)'])\n",
    "\n",
    "# Create a new dataframe to store the condensed data\n",
    "condensed_df = pd.DataFrame()\n",
    "\n",
    "# Identify unique patients\n",
    "unique_patients = transformed_df['Patient ID'].unique()\n",
    "\n",
    "# Create a single row for each patient\n",
    "for patient_id in unique_patients:\n",
    "    patient_data = transformed_df[transformed_df['Patient ID'] == patient_id]\n",
    "    \n",
    "    # Sort by timestamp to ensure consistent ordering\n",
    "    patient_data = patient_data.sort_values('Timestamp')\n",
    "    \n",
    "    # Create a new row for this patient\n",
    "    new_row = {\n",
    "        'Patient ID': patient_id,\n",
    "        'Gender': patient_data['Gender'].iloc[0]\n",
    "    }\n",
    "    \n",
    "    # Get the measures that we want to condense (all except Patient ID, Gender, Timestamp)\n",
    "    measures = [col for col in patient_data.columns if col not in ['Patient ID', 'Gender', 'Timestamp']]\n",
    "    \n",
    "    # For each measure, create 10 new columns (one for each year)\n",
    "    for measure in measures:\n",
    "        measure_values = patient_data[measure].values\n",
    "        for i, value in enumerate(measure_values):\n",
    "            new_row[f'{measure}_Year_{i+1}'] = value\n",
    "    \n",
    "    # Add to the condensed dataframe\n",
    "    condensed_df = pd.concat([condensed_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Output the condensed data\n",
    "output_dir = \"/transformed_data\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"condensed_vital_signs_dataset.csv\")\n",
    "condensed_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Condensed dataset created with {len(condensed_df)} rows.\")\n",
    "print(\"Sample of condensed data:\")\n",
    "print(condensed_df.head(5))\n",
    "print(condensed_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "full_df = pd.read_csv('/transformed_data/transformed_vital_signs_dataset.csv')\n",
    "\n",
    "# Filter for only 'High Risk' rows\n",
    "high_risk_df = full_df[full_df['Risk Category'] == 'High Risk']\n",
    "\n",
    "# Print the first 10 rows of the filtered dataset\n",
    "print(\"First 10 rows of the HIGH RISK patients:\")\n",
    "print(high_risk_df.head(1))\n",
    "\n",
    "# Print dataset info for High Risk only\n",
    "print(f\"\\nFiltered dataset shape: {high_risk_df.shape}\")\n",
    "print(f\"Number of unique Patient IDs (High Risk only): {high_risk_df['Patient ID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your condensed dataset\n",
    "df = pd.read_csv(\"/transformed_data/condensed_vital_signs_dataset.csv\")\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# Function to check if a column is numeric\n",
    "def is_numeric_column(df, column):\n",
    "    # Try to convert to numeric, return True if successful\n",
    "    try:\n",
    "        pd.to_numeric(df[column])\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "# Split features by year\n",
    "def create_time_series_features(df):\n",
    "    # Get all column names\n",
    "    cols = df.columns\n",
    "    \n",
    "    # Identify feature columns (those ending with Year_X)\n",
    "    feature_cols = [col for col in cols if '_Year_' in col]\n",
    "    \n",
    "    # Filter to keep only numeric columns\n",
    "    numeric_feature_cols = [col for col in feature_cols if is_numeric_column(df, col)]\n",
    "    \n",
    "    # Group features by year\n",
    "    year_groups = {}\n",
    "    for year in range(1, 11):\n",
    "        year_cols = [col for col in numeric_feature_cols if f'_Year_{year}' in col]\n",
    "        if year_cols:\n",
    "            year_groups[year] = year_cols\n",
    "            \n",
    "    return year_groups, numeric_feature_cols\n",
    "\n",
    "# Get numeric features grouped by year\n",
    "year_groups, all_feature_cols = create_time_series_features(df)\n",
    "print(f\"Found {len(all_feature_cols)} numeric feature columns\")\n",
    "\n",
    "# Create X (features) and y (target)\n",
    "# For example, predict vital signs in year 10 based on years 1-9\n",
    "X_cols = []\n",
    "for year in range(1, 10):  # Years 1-9\n",
    "    if year in year_groups:\n",
    "        X_cols.extend(year_groups[year])\n",
    "    \n",
    "y_cols = year_groups.get(10, [])  # Year 10 (handle case where year 10 might not exist)\n",
    "\n",
    "if not y_cols:\n",
    "    print(\"Warning: No numeric target columns found for year 10\")\n",
    "else:\n",
    "    print(f\"Using {len(X_cols)} input features to predict {len(y_cols)} target features\")\n",
    "\n",
    "    X = df[X_cols]\n",
    "    y = df[y_cols]\n",
    "\n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    print(\"Scaling completed successfully\")\n",
    "    \n",
    "    # Now you can proceed with your model building\n",
    "    # For example:\n",
    "    # model = RandomForestRegressor()\n",
    "    # model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
